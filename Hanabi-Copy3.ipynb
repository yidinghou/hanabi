{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Structure\n",
    "\n",
    "output policy will have a dimension of the sum of discard choices, play choices, and clue choices. clue choices has a cardinality of product of num players, num colors, and num numbers. In this case it will be 4+4+3*3*1= 17 element array\n",
    "\n",
    "Input layer will have a dimension of the sum of opp_hand, board, discard, self_clue, and opp_clue.\n",
    "opp_hand will be dummy encoding of the number of cards in the deck. board is the number of playable cards. self_clue is product of num of cards_in_hand, num of colors, and num of value, so 4*3*3 = 36. Same with opp_clue\n",
    "\n",
    "Input layer = 15+9+15+36+36 = 111 node\n",
    "also add in num bombs, number of cards left, number of clues, 4, 15-4*2=7, 8\n",
    "\n",
    "total input layer = 111+4+7+8 = 130\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yidin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing prior model\n",
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1c82b20f95ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No prior model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mscores_best\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselfplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-1c82b20f95ae>\u001b[0m in \u001b[0;36mselfplay\u001b[1;34m(model, EPISODES, memory, cards)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"P2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mboard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBoard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mdeck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mp1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeck\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from memory import Memory\n",
    "from Game import *\n",
    "from Player import *\n",
    "import pickle\n",
    "from Model import *\n",
    "import config\n",
    "\n",
    "def selfplay(model, EPISODES, memory, cards):\n",
    "    scores = []\n",
    "    colors = [\"R\",\"B\",\"Y\",'G',\"W\"]\n",
    "    numbers = [1,1,1,2,2,3,3,4,4,5]\n",
    "\n",
    "    cards = [Card(value, color) for color in colors for value in numbers]\n",
    "    deck = Deck(cards = cards)\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        print(e)\n",
    "        deck = Deck(cards=cards)\n",
    "        p1 = Player(\"P1\", deck.copy(), model)\n",
    "        p2 = Player(\"P2\", deck.copy(), model)\n",
    "\n",
    "        board = Board(colors, deck.copy())\n",
    "        deck.shuffle()\n",
    "        p1.draw(deck.deal(4))\n",
    "        p2.draw(deck.deal(4))\n",
    "\n",
    "        players = [p1, p2]\n",
    "        turn = 0\n",
    "        curr = players[(turn) % 2]\n",
    "        opp = players[(turn + 1) % 2]\n",
    "        currGS = GS(board, deck, curr, [opp], [\"Start\"], turn)\n",
    "        done = 0\n",
    "\n",
    "        while done == 0:\n",
    "            curr = players[(turn) % 2]\n",
    "            opp = players[(turn + 1) % 2]\n",
    "            turn += 1\n",
    "            curr.see_players([opp])\n",
    "            curr.see_board(board)\n",
    "            opp.see_players([curr])\n",
    "            opp.see_board(board)\n",
    "\n",
    "            output = p1.act(currGS, tau=1)\n",
    "            currGS, value, done = currGS.takeAction(output[0])\n",
    "            memory.commit_stmemory(currGS, output[1])\n",
    "\n",
    "            if done == 1:\n",
    "                if memory != None:\n",
    "                    for move in memory.stmemory:\n",
    "                        move[\"value\"] = value\n",
    "\n",
    "                memory.commit_ltmemory()\n",
    "\n",
    "        scores.append(value)\n",
    "    return (scores)\n",
    "\n",
    "\n",
    "memory = Memory(config.MEMORY_SIZE)\n",
    "colors = [\"R\", \"B\", \"Y\", 'G', \"W\"]\n",
    "numbers = [1, 1, 1, 2, 2, 3, 3, 4, 4, 5]\n",
    "\n",
    "cards = [Card(value, color) for color in colors for value in numbers]\n",
    "deck = Deck(cards=cards)\n",
    "\n",
    "best = nn_model()\n",
    "try:\n",
    "    best_weights = pickle.load(open(\"best_weights.p\", \"rb\" ))\n",
    "    best.model.set_weights(best_weights)\n",
    "    print(\"importing prior model\")\n",
    "except:\n",
    "    print(\"No prior model\")\n",
    "    \n",
    "scores_best = selfplay(best.model, 10, memory, cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "501/501 [==============================] - 0s 686us/step - loss: 0.6869 - policy_head_loss: 0.3976 - value_head_loss: 0.9762\n",
      "Epoch 1/1\n",
      "501/501 [==============================] - 0s 155us/step - loss: 0.6859 - policy_head_loss: 0.3956 - value_head_loss: 0.9762\n",
      "Epoch 1/1\n",
      "501/501 [==============================] - 0s 185us/step - loss: 0.6863 - policy_head_loss: 0.3964 - value_head_loss: 0.9762\n",
      "Epoch 1/1\n",
      "501/501 [==============================] - 0s 177us/step - loss: 0.6855 - policy_head_loss: 0.3948 - value_head_loss: 0.9762\n",
      "Epoch 1/1\n",
      "501/501 [==============================] - 0s 149us/step - loss: 0.6850 - policy_head_loss: 0.3937 - value_head_loss: 0.9762\n",
      "Epoch 1/1\n",
      "501/501 [==============================] - 0s 133us/step - loss: 0.6851 - policy_head_loss: 0.3939 - value_head_loss: 0.9762\n",
      "Epoch 1/1\n",
      "501/501 [==============================] - 0s 174us/step - loss: 0.6850 - policy_head_loss: 0.3937 - value_head_loss: 0.9762\n",
      "Epoch 1/1\n",
      "501/501 [==============================] - 0s 139us/step - loss: 0.6845 - policy_head_loss: 0.3928 - value_head_loss: 0.9762\n",
      "Epoch 1/1\n",
      "501/501 [==============================] - 0s 165us/step - loss: 0.6846 - policy_head_loss: 0.3929 - value_head_loss: 0.9762\n",
      "Epoch 1/1\n",
      "501/501 [==============================] - 0s 116us/step - loss: 0.6845 - policy_head_loss: 0.3928 - value_head_loss: 0.9762\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "not better\n"
     ]
    }
   ],
   "source": [
    "new = nn_model()\n",
    "new.model.set_weights(best.model.get_weights())\n",
    "\n",
    "for i in range(config.TRAINING_LOOPS):\n",
    "    minibatch = random.sample(memory.ltmemory, min(config.BATCH_SIZE, len(memory.ltmemory)))\n",
    "\n",
    "    training_states = np.array([row['state'].nn_input for row in minibatch])\n",
    "    training_targets = {'value_head': np.array([row['value'] for row in minibatch])\n",
    "        , 'policy_head': np.array([row['AV'] for row in minibatch])}\n",
    "\n",
    "    fit = new.model.fit(training_states, [training_targets[\"policy_head\"], training_targets[\"value_head\"]],\n",
    "                        epochs=config.EPOCHS, verbose=1, validation_split=0, batch_size=32)\n",
    "\n",
    "scores_new = selfplay(new.model, 10, memory, cards)\n",
    "\n",
    "if sum(scores_new) > sum(scores_best):\n",
    "    best_weights = new.model.get_weights()\n",
    "    pickle.dump(best_weights, open(\"best_weights.p\", \"wb\" ) )\n",
    "    best.model.set_weights(best_weights)\n",
    "    print(\"better model\")\n",
    "else:\n",
    "    print(\"not better\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0.04, 0, 0, 0, 0.04, 0, 0, 0.04, 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04, 0, 0, 0, 0, 0, 0, 0.04, 0, 0.04]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(memory.ltmemory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AV': array([0.02040816, 0.        , 0.02040816, 0.        , 0.02040816,\n",
       "        0.        , 0.        , 0.        , 0.12244898, 0.        ,\n",
       "        0.08163265, 0.12244898, 0.36734694, 0.24489796]),\n",
       " 'state': <Game.GS at 0x185e58bf860>,\n",
       " 'value': 0.04}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (memory.ltmemory)[-2]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Start',\n",
       " '1: Action P1 discarded: 3R',\n",
       " '2: Action P2 discarded: 1Y',\n",
       " '3: Action P1 discarded: 3G',\n",
       " '4: Action P2 discarded: 4R',\n",
       " '5: Action P1 discarded: 4B',\n",
       " '6: Action P2 discarded: 2G',\n",
       " '7: Action P1 discarded: 4Y',\n",
       " '8: Action P2 discarded: 3W',\n",
       " '9: Action P1 discarded: 1W',\n",
       " '10: Action P2 discarded: 4G',\n",
       " '11: Action P1 discarded: 5W',\n",
       " '12: Action P2 discarded: 3R',\n",
       " '13: Action P1 discarded: 4W',\n",
       " '14: Action P2 discarded: 1R',\n",
       " '15: Action P1 discarded: 2B',\n",
       " '16: Action P2 discarded: 3B',\n",
       " '17: Action P1 discarded: 3B',\n",
       " '18: Action P2 discarded: 2W',\n",
       " '19: Action P1 discarded: 5Y',\n",
       " '20: Action P2 discarded: 4G',\n",
       " '21: Action P1 discarded: 2Y',\n",
       " '22: Action P2 discarded: 4Y',\n",
       " '23: Action P1 discarded: 1B',\n",
       " '24: Action P2 discarded: 5B',\n",
       " '25: Action P1 discarded: 2Y',\n",
       " '26: Action P2 discarded: 1R',\n",
       " '27: Action P1 discarded: 1R',\n",
       " '28: Action P2 discarded: 1Y',\n",
       " '29: Action P1 discarded: 2R',\n",
       " '30: Action P2 discarded: 2R',\n",
       " '31: Action P1 discarded: 3G',\n",
       " '32: Action P2 discarded: 1W',\n",
       " '33: Action P1 gave clue: R to P2',\n",
       " '34: Action P2 played: 5R',\n",
       " '35: Action P1 discarded: 4B',\n",
       " '36: Action P2 discarded: 2G',\n",
       " '37: Action P1 discarded: 1W',\n",
       " '38: Action P2 discarded: 3Y',\n",
       " '39: Action P1 discarded: 4W',\n",
       " '40: Action P2 gave clue: 1 to P1',\n",
       " '41: Action P1 gave clue: 3 to P2',\n",
       " '42: Action P2 played: 1G',\n",
       " '43: Action P1 gave clue: Y to P2',\n",
       " '44: Action P2 gave clue: Y to P1',\n",
       " '45: Action P1 gave clue: B to P2',\n",
       " '46: Action P2 gave clue: Y to P1',\n",
       " '47: Action P1 discarded: 2B',\n",
       " '48: Action P2 gave clue: B to P1',\n",
       " '49: Action P1 discarded: 3Y',\n",
       " '50: Action P2 gave clue: Y to P1',\n",
       " '51: Action P1 gave clue: 1 to P2',\n",
       " '52: Action P2 gave clue: 3 to P1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['state'].log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_weights = new.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(list(memory.ltmemory).copy(), open(\"besdfgs.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(memory.ltmemory).copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
